---
title: "ST558 - Project 2 - Predictive Modeling"
author: "Jasmine Wang & Ilana Feldman"
date: "10/31/2021"
params:
  channel: "lifestyle"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.path = "../images/", echo = TRUE, message = FALSE, warning = FALSE)

```

# Introduction

briefly describes the data 
briefly describes the variables you have to work with (describe what you want to use)

purpose of the analysis
methods you will use to model the response (more details in modeling section)

61 variables (only 58 predictive variables, 2 non-predictive), target response is "shares".

# Data


Predictors                    | Attribute Information                              | Type   
----------------------------- | -------------------------------------------------- | -------------------------
`kw_avg_avg`                  | Average keyword (average shares)                   | number
`LDA_02`                      | Closeness to LDA topic 2                           | ratio
`weekday_is_monday`           | Was the article published on a Monday?             | boolean
`weekday_is_tuesday`          | Was the article published on a Tuesday?            | boolean
`weekday_is_wednesday`        | Was the article published on a Wednesday?          | boolean
`weekday_is_thursday`         | Was the article published on a Thursday?           | boolean
`weekday_is_friday`           | Was the article published on a Friday?             | boolean
`weekday_is_saturday`         | Was the article published on a Saturday?           | boolean
`weekday_is_sunday`           | Was the article published on a Sunday?             | boolean
`self_reference_avg_sharess`  | Average shares of referenced articles in Mashable  | number
`n_non_stop_unique_tokens`    | Rate of unique non-stop words in the content       | ratio
`average_token_length`        | Average length of the words in the content         | number
`n_tokens_content`            | Number of words in the content                     | number
`n_tokens_title`              | Number of words in the title                       | number
`global_subjectivity`         | Text subjectivity                                  | ratio
`num_imgs`                    | Number of images                                   | number

I created a binary response variable, 0 if shares < 1400, 1 if shares > 1400. "class_shares" (can use it in EDA)

I created a categorical variable grouped all binary variables, monday, tuesday, ..., sunday, together. "dayweek"
if dayweek = 1, it's Monday, 2 is tuesday, 3 is wednesday, ..., 7 is sunday. 

I created a log(shares) variable and use it as response instead of shares. In office hour, a lot of people say this improved fit a little better. 

This analysis is based on the `r params$channel` channel popularity.


```{r eval=TRUE}
library(tidyverse)
library(knitr)
library(caret)
library(corrplot)
library(ggplot2)
library(gbm)

allnews <- read_csv("../_Data/OnlineNewsPopularity.csv", 
                 col_names = TRUE)

########KNIT with parameters!!!!!!!!!channels is in quotes!!!!Need to use it with quotes!!!!!!!!!!!!!!!!!!!!!!!!

channels <- paste0("data_channel_is_", params$channel)
subnews <- allnews[allnews[, channels] == 1, ]

news <- subnews %>% select(
  -data_channel_is_lifestyle, -data_channel_is_entertainment, -data_channel_is_bus, -data_channel_is_socmed, 
  -data_channel_is_tech, -data_channel_is_world, -url, -timedelta)
#################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
dim(news)

diffday <- news %>% mutate(log.shares = log(shares),
                           class_shares = if_else(shares < 1400, 0, 1),
                           dayweek = if_else(weekday_is_monday == 1, 1,
                                    if_else(weekday_is_tuesday == 1, 2,
                                    if_else(weekday_is_wednesday == 1, 3,
                                    if_else(weekday_is_thursday == 1, 4,
                                    if_else(weekday_is_friday == 1, 5,
                                    if_else(weekday_is_saturday == 1, 6, 7))))))
                           )

sel_data <- diffday %>% select(class_shares, shares, log.shares, dayweek, 
                               kw_avg_avg, 
                               LDA_00, LDA_01, LDA_02, LDA_03, LDA_04, 
                               weekday_is_monday, weekday_is_tuesday, weekday_is_wednesday,
                               weekday_is_thursday, weekday_is_friday, weekday_is_saturday, weekday_is_sunday,
                               self_reference_avg_sharess, 
                               n_non_stop_unique_tokens, average_token_length, 
                               n_tokens_content, n_tokens_title, global_subjectivity, 
                               num_imgs)

set.seed(388588)
sharesIndex <- createDataPartition(sel_data$shares, p = 0.7, list = FALSE)
train <- sel_data[sharesIndex, ]
test <- sel_data[-sharesIndex, ]

train1 <- train %>% select(-class_shares, -shares, 
                           -weekday_is_monday, -weekday_is_tuesday, -weekday_is_wednesday, -weekday_is_thursday, 
                           -weekday_is_friday, -weekday_is_saturday, -weekday_is_sunday, -LDA_00, -LDA_01, -LDA_03, -LDA_04)
train1
test1 <- test %>% select(-class_shares, -shares, 
                         -weekday_is_monday, -weekday_is_tuesday, -weekday_is_wednesday, -weekday_is_thursday, 
                         -weekday_is_friday, -weekday_is_saturday, -weekday_is_sunday, -LDA_00, -LDA_01, -LDA_03, -LDA_04) #keep log.shares
```

# Exploratory Data Analysis

The `r params$channel` channel has `r dim(train)[1]` articles collected. Now let us take a look at the relationships between our response and the predictors with some numerical summaries and plots. 

## Numerical Summaries

Table 1 shows the popularity of the news articles on different days of the week. I classified number of shares greater than 1400 in a day as "popular" and number of shares less than 1400 in a day as "unpopular". We can see the total number of articles from `r params$channel` channel falls into different categories on different days of the week for 709 days. 

Table 2 shows the average shares of the articles on different days of the week. Here, we can see a potential problem for our analysis later. Median shares are all very different from the average shares on any day of the week. Recall that median is a robust measure for center. It is robust to outliers in the data. On the contrary, mean is also a measure of center but it is not robust to outliers. Mean measure can be influenced by potential outliers. 

In addition, Table 2 also shows the standard deviation of shares is huge for any day of the week. They are potentially larger than the average shares. This tells us the variance of shares for any day is huge. We know a common variance stabilizing transformation to deal with increasing variance of the response variable, that is, the log-transformation, which could help us on this matter. Therefore, Table 2 again shows after the log-transformation of shares, the mean values are similar to their corresponding median values, and their standard deviations are much smaller than before relatively speaking.

Table 3 shows the numerical summaries of average keywords from `r params$channel` channel in mashable.com on different days of the week. Table 4 shows the numerical summaries of average shares of referenced articles in mashable.com on different days of the week. 

Table 5 checks the numerical summaries of the `global_subjectivity` variable between popular and unpopular articles, to see if there's any difference or a higher variation in subjectivity in popular articles. Text subjectivity is a value between 0 and 1, so there isn't any need for transformation.

Table 6 checks the numerical summaries of the image count per article on different days of the week, to see if there is a noticeable difference in image count on weekends versus weekdays across all channels, or only certain ones. Much like in table 2, the mean is smaller than the standard deviation for most of the days of the week, and the solution isn't as straightforward, since many of the articles don't have any images at all. I'll additionally include a log transformation of `images + 1` to account for this.

```{r eval=TRUE}
# contingency table
edadata <- train
edadata$class.shares <- cut(edadata$class_shares, 2, c("Unpopular","Popular"))
edadata$day.week <- cut(edadata$dayweek, 7, c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))
table(edadata$class.shares, edadata$day.week) %>% kable(caption = "Table 1. Popularity on Day of the Week")

edadata %>% group_by(day.week) %>% summarise(
  Avg.shares = mean(shares), Sd.shares = sd(shares), Median.shares = median(shares), 
  Avg.logshares = mean(log.shares), Sd.logshares = sd(log.shares), Median.logshares = median(log.shares)) %>% 
  kable(digits = 4, caption = "Table 2. Average Shares vs. Average Log(shares) on Day of the Week")

edadata %>% group_by(day.week) %>% summarise(
  Avg.keyword = mean(kw_avg_avg), Sd.keyword = sd(kw_avg_avg), Median.keyword = median(kw_avg_avg), 
  IQR.keyword = IQR(kw_avg_avg)) %>% 
  kable(digits = 4, caption = "Table 3. Summary of Average Keywords on Day of the Week")

edadata %>% group_by(day.week) %>% summarise(
  Avg.reference = mean(self_reference_avg_sharess), Sd.reference = sd(self_reference_avg_sharess), 
  Median.reference = median(self_reference_avg_sharess), IQR.reference = IQR(self_reference_avg_sharess)) %>% 
  kable(digits = 4, caption = "Table 4. Summary of Average shares of referenced articles in Mashable on Day of the Week")

edadata %>% group_by(class.shares) %>% summarize(
  Avg.subjectivity = mean(global_subjectivity), Sd.subjectivity = sd(global_subjectivity), 
  Median.subjectivity = median(global_subjectivity)) %>% kable(digits = 4, caption = "Table 5. Comparing Global Subjectivity between Popular and Unpopular Articles")

edadata %>% group_by(day.week) %>% summarize(
  Avg.images = mean(num_imgs), Sd.images = sd(num_imgs), Median.images = median(num_imgs), Avg.log.images = mean(log(num_imgs + 1)), Sd.log.images = sd(log(num_imgs + 1)), Median.log.images = median(log(num_imgs + 1))) %>%
  kable(digits = 4, caption = "Table 6. Comparing Image Counts by the Day of the Week")
```

## Visualizations

Graphical presentation is a great tool used to visualize the relationships between the predictors and the number of shares (or log number of shares). Below we will see some plots that tell us stories between those variables.

### Correlation Plot

Figure 1 shows the correlations between the variables, both the response and the predictors, which will be used in the regression models as well as the ensemble models for predicting the number of shares. Notice that there may be some collinearity among the predictor variables. 

```{r eval=TRUE}
# keep log-shares
#corplt <- train %>% select(-class_shares, -weekday_is_monday, -weekday_is_tuesday, -weekday_is_wednesday,
#                           -weekday_is_thursday, -weekday_is_friday, -weekday_is_saturday, -weekday_is_sunday) 
file.name <- paste0("../images/", params$channel, 1, ".png")
png(filename = file.name)

correlation <- cor(train1, method="spearman")

corrplot(correlation, type = "upper", tl.pos = "lt")
corrplot(correlation, type = "lower", method = "number", add = TRUE, diag = FALSE, tl.pos = "n", 
         cex = 0.8,
         title="Figure 1. Correlations Between the Variables")
dev.off()
```

### Boxplot 

Figure 2 shows the number of shares across different days of the week. Here, due to the huge number of large-valued outliers, I capped the number of shares to 10,000 so that we can see the medians and the interquartile ranges for different days of the week. Figure 2 coincides with the findings in Table 2 that the variance of shares is huge across days of the week, and the mean values of shares across different days are driven by larged-valued outliers. Therefore, those mean values of shares are not close to the median values of shares for each day of the week. The median number of shares seems to be bigger during weekend than weekdays.

```{r eval=TRUE}
file.name <- paste0("../images/", params$channel, 2, ".png")
png(filename = file.name)

boxplot1 <- ggplot(data = edadata, aes(x = day.week, y = shares))
boxplot1 + geom_boxplot(fill = "white", outlier.shape = NA) + 
  coord_cartesian(ylim=c(0, 10000)) + 
  geom_jitter(aes(color = day.week), size = 1) + 
  labs(x = "Day of the Week", y = "Number of Shares", 
       title = "Figure 2. Number of shares across different days of the week") + 
  scale_color_discrete(name = "Day of the Week") +
  theme(axis.text.x = element_text(angle = 45, size = 10), 
        axis.text.y = element_text(size = 10), 
        axis.title.x = element_text(size = 15), 
        axis.title.y = element_text(size = 15), 
        legend.key.size = unit(1, 'cm'), 
        legend.text = element_text(size = 13), 
        title = element_text(size = 14))
dev.off()
```

### Barplot

Figure 3 shows the popularity of the closeness to a top LDA topic for the `r params$channel` channel on mashable.com on any day of the week. The measurements of the different LDA topics are in ratios, and these are the mean ratios calculated for the specific day of thte week for that topic across 709 days of collections of data in mashable.com. These mean ratios are further classified into a "popular" group and an "unpopular" group according to their number of shares. 

Some mean ratios of a LDA topic do not seem to vary over the days of a week while other mean ratios of LDA topics vary across different days of the week. Note, when we dicotomize a continuous variable into different groups, we lose information about that variable. Here, I just want to show you whether or not the mean ratios of a LDA topic differ across time for different levels of shares. The classified version of number of shares will not be used to fit in a model later.

```{r eval=TRUE}
file.name <- paste0("../images/", params$channel, 3, ".png")
png(filename = file.name)

b.plot1 <- edadata %>% group_by(day.week, class.shares) %>% 
  summarise(LDA_0=mean(LDA_00), LDA_1=mean(LDA_01), LDA_2=mean(LDA_02), LDA_3=mean(LDA_03), LDA_4=mean(LDA_04))

b.plot2 <- b.plot1 %>% pivot_longer(cols = 3:7, names_to = "LDA.Topic", values_to = "avg.LDA")

barplot1 <- ggplot(data = b.plot2, aes(x = day.week, y = avg.LDA, fill = LDA.Topic))
barplot1 + geom_bar(stat = "identity", position = "stack") + 
  labs(x = "Day of the Week", y = "Closeness to Top LDA Topic", 
       title = "Figure 3. Popularity of Top LDA Topic on Day of the Week") + 
  scale_fill_discrete(name = "LDA Topic") + 
  theme(axis.text.x = element_text(angle = 45, size = 10), 
        axis.text.y = element_text(size = 10), 
        axis.title.x = element_text(size = 13), 
        axis.title.y = element_text(size = 13), 
        legend.key.size = unit(1, 'cm'), 
        legend.text = element_text(size = 13), 
        title = element_text(size = 13)) + 
  facet_wrap(~ class.shares)
dev.off()
```

### Line Plot

Here, Figure 4 shows the same measurements as in Figure 3 but in line plot which we can see how the patterns of the mean ratios of a LDA topic vary or not vary across time in different popularity groups more clearly. Again, some mean ratios do not seem to vary across time and across popularity groups while some other mean ratios vary across time and popularity groups for articles in the `r params$channel` channel.

```{r eval=TRUE}
file.name <- paste0("../images/", params$channel, 4, ".png")
png(filename = file.name)

l.plot1 <- edadata %>% group_by(day.week, class.shares) %>% 
  summarise(LDA_0=mean(LDA_00), LDA_1=mean(LDA_01), LDA_2=mean(LDA_02), LDA_3=mean(LDA_03), LDA_4=mean(LDA_04))
l.plot1

l.plot2 <- l.plot1 %>% pivot_longer(cols = 3:7, names_to = "LDA.Topic", values_to = "avg.LDA")
l.plot2

lineplot1 <- ggplot(data = l.plot2, aes(x = day.week, y = avg.LDA, group = LDA.Topic))
lineplot1 + geom_line(aes(color = LDA.Topic), lwd = 2) + 
  labs(x = "Day of the Week", y = "Closeness to LDA Topic", 
       title = "Figure 4. Popularity of LDA Topic on Day of the Week") + 
  scale_color_discrete(name = "LDA Topic") +
  theme(axis.text.x = element_text(angle = 45, size = 10), 
        axis.text.y = element_text(size = 10), 
        axis.title.x = element_text(size = 15), 
        axis.title.y = element_text(size = 15), 
        legend.key.size = unit(1, 'cm'), 
        legend.text = element_text(size = 13), 
        title = element_text(size = 13)) +
  facet_wrap(~ class.shares)
dev.off()
```

### Scatterplot

Figure 5 shows the relationship between average keyword and log-transformed number of shares for articles in the `r params$channel` channel across different days of the week. In the news popularity study, it says average keyword is the most important predictor in the models they used which accounted for the most variation in the data. Therefore, we are interested to see how average keyword is correlated with log shares. The different colored linear regression lines indicate different days of the week. If it is an upward trend, it shows positive linear relationship. If it is a downward trend, it shows a negative linear relationship. More tilted the line is, much stronger the relationship is regardless of positive or negative.

Figure 6 is similar, except it compares the log-transformed number of shares to the log-transformed images in the article. As noted previously, both of these variables do not behave properly in a linear model due to the existence of extreme outliers in the data.

```{r eval=TRUE}
file.name <- paste0("../images/", params$channel, 5, ".png")
png(filename = file.name)

scatter1 <- ggplot(data = edadata, aes(x = kw_avg_avg, y = log.shares, color = day.week)) #y=kw_avg_max
scatter1 + geom_point(size = 2) + #aes(shape = class.shares)
  scale_color_discrete(name = "Day of the Week") + 
  coord_cartesian(xlim=c(0, 10000)) +
  geom_smooth(method = "lm", lwd = 2) + 
  labs(x = "Average Keywords", y = "log(number of shares)", 
       title = "Figure 5. Average Keywords vs Log Number of Shares") + 
  theme(axis.text.x = element_text(size = 10), 
        axis.text.y = element_text(size = 10), 
        axis.title.x = element_text(size = 15), 
        axis.title.y = element_text(size = 15), 
        legend.key.size = unit(1, 'cm'), 
        legend.text = element_text(size = 13), 
        title = element_text(size = 13))
dev.off()
```

```{r eval = TRUE}
file.name <- paste0("../images/", params$channel, 6, ".png")
png(filename = file.name)

scatter2 <- ggplot(data = edadata, aes(x = log(num_imgs + 1), y = log.shares, color = day.week))
scatter2 + geom_point(size = 2) +
  scale_color_discrete(name = "Day of the Week") + 
  geom_smooth(method = "lm", lwd = 2) + 
  labs(x = "log(number of images)", y = "log(number of shares)", 
       title = "Figure 5. Log Number of Images vs Log Number of Shares") + 
  theme(axis.text.x = element_text(size = 10), 
        axis.text.y = element_text(size = 10), 
        axis.title.x = element_text(size = 15), 
        axis.title.y = element_text(size = 15), 
        legend.key.size = unit(1, 'cm'), 
        legend.text = element_text(size = 13), 
        title = element_text(size = 13))
dev.off()
```

### QQ Plots

To justify the usage of the log transformations for shares and images, we'll show the QQ plot of each over the `r params$channel` channel in figures 7a, 7b, 7c, and 7d. We're aiming for something close to a straight line, which would indicate that the data is approximately normal in its distribution and does not need further standardization.

```{r eval = TRUE}
file.name <- paste0("../images/", params$channel, "7a.png")
png(filename = file.name)

ggplot(edadata) + geom_qq(aes(sample = shares)) + geom_qq_line(aes(sample = shares)) + 
  labs(x = "Theoretical Quantiles", y = "Share Numbers", 
       title = "Figure 7a. QQ Plot for Non-Transformed Shares") +
    theme(axis.text.x = element_text(size = 10), 
        axis.text.y = element_text(size = 10), 
        axis.title.x = element_text(size = 15), 
        axis.title.y = element_text(size = 15), 
        legend.key.size = unit(1, 'cm'), 
        legend.text = element_text(size = 13), 
        title = element_text(size = 13))
dev.off()
```

```{r eval = TRUE}
file.name <- paste0("../images/", params$channel, "7b.png")
png(filename = file.name)

ggplot(edadata) + geom_qq(aes(sample = log(shares))) + geom_qq_line(aes(sample = log(shares))) +
    labs(x = "Theoretical Quantiles", y = "Log(Share Numbers)", 
       title = "Figure 7b. QQ Plot for Log-Transformed Shares") +
    theme(axis.text.x = element_text(size = 10), 
        axis.text.y = element_text(size = 10), 
        axis.title.x = element_text(size = 15), 
        axis.title.y = element_text(size = 15), 
        legend.key.size = unit(1, 'cm'), 
        legend.text = element_text(size = 13), 
        title = element_text(size = 13))
dev.off()
```

```{r eval = TRUE}
file.name <- paste0("../images/", params$channel, "7c.png")
png(filename = file.name)

ggplot(edadata) + geom_qq(aes(sample = num_imgs)) + geom_qq_line(aes(sample = num_imgs)) + 
  labs(x = "Theoretical Quantiles", y = "Image Numbers", 
       title = "Figure 7c. QQ Plot for Non-Transformed Image Numbers") +
    theme(axis.text.x = element_text(size = 10), 
        axis.text.y = element_text(size = 10), 
        axis.title.x = element_text(size = 15), 
        axis.title.y = element_text(size = 15), 
        legend.key.size = unit(1, 'cm'), 
        legend.text = element_text(size = 13), 
        title = element_text(size = 13))
dev.off()
```

```{r eval = TRUE}
file.name <- paste0("../images/", params$channel, "7d.png")
png(filename = file.name)

ggplot(edadata) + geom_qq(aes(sample = log(num_imgs + 1))) + geom_qq_line(aes(sample = log(num_imgs + 1))) +
    labs(x = "Theoretical Quantiles", y = "Log(Image Numbers)", 
       title = "Figure 7d. QQ Plot for Log-Transformed Image Numbers") +
    theme(axis.text.x = element_text(size = 10), 
        axis.text.y = element_text(size = 10), 
        axis.title.x = element_text(size = 15), 
        axis.title.y = element_text(size = 15), 
        legend.key.size = unit(1, 'cm'), 
        legend.text = element_text(size = 13), 
        title = element_text(size = 13))
dev.off()
```

Whether it's appropriate to perform a logarithmic transformation on the number of images is somewhat less clear than for the number of shares.

# Modeling

## Linear Regression

The linear regression process takes a matrix of all of the predictor variables we've chosen and compares their values to each of the corresponding values of the response variable, `log.shares`. This allows us to calculate the most accurate linear combination of the predictor variables to make up the response variable. We can choose a variety of sets of predictors and compare their Root Mean Square Errors, R-Squared values, and Mean Absolute Errors to see which one is the strongest model. Below, we've fit multiple linear models that include all of our variables and various combinations of interaction terms and/or quadratic terms.

```{r eval=TRUE}
# using train1, dayweek is numeric, no class_shares
#train1 <- train %>% select(-class_shares, -shares) #keep log.shares
#test1 <- test %>% select(-class_shares, -shares) #keep log.shares
train1$dayweek <- as.factor(train1$dayweek)
test1$dayweek <- as.factor(test1$dayweek)
preProcValues <- preProcess(train1, method = c("center", "scale"))
trainTransformed <- predict(preProcValues, train1)
testTransformed <- predict(preProcValues, test1)

cv_fit1 <- train(log.shares ~ . , 
                 data=trainTransformed,
                 method = "lm",
                 trControl = trainControl(method = "cv", number = 10))
summary(cv_fit1)

cv_fit2 <- train(log.shares ~ . +I(n_tokens_content^2)+ kw_avg_avg:num_imgs + 
                   average_token_length:global_subjectivity + 
                   dayweek:self_reference_avg_sharess ,
                 data=trainTransformed,
                 method = "lm",
                 trControl = trainControl(method = "cv", number = 10))
summary(cv_fit2)

cv_fit3 <- train(log.shares ~ . + I(n_tokens_content^2) + I(self_reference_avg_sharess^2) + 
                 kw_avg_avg:num_imgs + average_token_length:global_subjectivity, 
                 data=trainTransformed,
                 method = "lm",
                 trControl = trainControl(method = "cv", number = 10))
summary(cv_fit3)

cv_fit4 <- train(log.shares ~ . - num_imgs + I(log(num_imgs + 1)) + I(n_tokens_content^2) +
                 I(self_reference_avg_sharess^2) + kw_avg_avg:I(log(num_imgs + 1)) +
                 average_token_length:global_subjectivity, 
                 data=trainTransformed,
                 method = "lm",
                 trControl = trainControl(method = "cv", number = 10))
summary(cv_fit4)

result_tab <- data.frame(t(cv_fit1$results),t(cv_fit2$results), t(cv_fit3$results), t(cv_fit4$results))
colnames(result_tab) <- c("Model 1","Model 2", "Model 3", "Model 4")
rownames(result_tab) <- c("intercept", "RMSE", "Rsquared", "MAE", "RMSESD", "RsquaredSD", "MAESD")

kable(result_tab, digits = 4, caption = "Cross Validation - Comparisons of the models in training set")

pred1 <- predict(cv_fit1, newdata = testTransformed)
pred2 <- predict(cv_fit2, newdata = testTransformed)
pred3 <- predict(cv_fit3, newdata = testTransformed)
pred4 <- predict(cv_fit4, newdata = testTransformed)
cv_rmse1 <- postResample(pred1, obs = testTransformed$log.shares)
cv_rmse2 <- postResample(pred2, obs = testTransformed$log.shares)
cv_rmse3 <- postResample(pred3, obs = testTransformed$log.shares)
cv_rmse4 <- postResample(pred4, obs = testTransformed$log.shares)
result2 <- rbind(cv_rmse1, cv_rmse2, cv_rmse3, cv_rmse4)
row.names(result2) <- c("Model 1","Model 2", "Model 3", "Model 4")
kable(result2, digits = 4, caption = "Table ###. Cross Validation - Model Predictions on Test Set")

```

## Random Forest 

The bootstrap approach to fitting a tree model involves resampling our data and fitting a tree to each sample, and then averaging the resulting predictions of each of those models. The random forest approach adds an extra step for each of these samples, where only a random subset of the predictor variables is chosen each time, in order to reduce the correlation between each of the trees. We don't have to worry about creating dummy variables for categorical variables, because our data already comes in an entirely numeric form.

```{r eval=TRUE}
train2 <- train %>% select(-class_shares, -shares, -dayweek, -LDA_00, -LDA_01, -LDA_03, -LDA_04)
test2 <- test %>% select(-class_shares, -shares, -dayweek, -LDA_00, -LDA_01, -LDA_03, -LDA_04)
train2
preProcValues <- preProcess(train2, method = c("center", "scale"))
trainTransformed <- predict(preProcValues, train2)
testTransformed <- predict(preProcValues, test2)

random_forest <- train(log.shares ~ ., data = trainTransformed,
    method = "rf",
    trControl = trainControl(method = "cv", number = 5),
    tuneGrid = data.frame(mtry = 1:5))

random_forest_predict <- predict(random_forest, newdata = testTransformed)
rf_rmse <- postResample(random_forest_predict, obs = testTransformed$log.shares)
rf_rmse
```

## Boosted Tree




short but reasonably thorough explanation of the ensemble model you are using


Predictors                    | Attribute Information                              | Type   
----------------------------- | -------------------------------------------------- | -------------------------
`kw_avg_avg`                  | Average keyword (average shares)                   | number
`LDA_02`                      | Closeness to LDA topic 2                           | ratio
`weekday_is_monday`           | Was the article published on a Monday?             | boolean
`weekday_is_tuesday`          | Was the article published on a Tuesday?            | boolean
`weekday_is_wednesday`        | Was the article published on a Wednesday?          | boolean
`weekday_is_thursday`         | Was the article published on a Thursday?           | boolean
`weekday_is_friday`           | Was the article published on a Friday?             | boolean
`weekday_is_saturday`         | Was the article published on a Saturday?           | boolean
`weekday_is_sunday`           | Was the article published on a Sunday?             | boolean
`self_reference_avg_sharess`  | Average shares of referenced articles in Mashable  | number
`n_non_stop_unique_tokens`    | Rate of unique non-stop words in the content       | ratio
`average_token_length`        | Average length of the words in the content         | number
`n_tokens_content`            | Number of words in the content                     | number
`n_tokens_title`              | Number of words in the title                       | number
`global_subjectivity`         | Text subjectivity                                  | ratio
`num_imgs`                    | Number of images                                   | number


```{r eval=TRUE}
#expand.grid(n.trees = c(25, 50, 100, 150, 200), interaction.depth = 1:4, shrinkage = 0.1, n.minobsinnode = 10)
boosted_tree <- train(log.shares ~ . , data = trainTransformed,
      method = "gbm", 
      trControl = trainControl(method = "cv", number = 10), #method="repeatedcv", repeats=5
      tuneGrid = expand.grid(n.trees = c(25, 50, 75, 100), interaction.depth = 1:4, shrinkage = 0.1, n.minobsinnode = 10),
      verbose = FALSE)
boosted_tree
boosted_tree_predict <- predict(boosted_tree, newdata = testTransformed)

boost_rmse <- postResample(boosted_tree_predict, obs = testTransformed$log.shares)

result2 <- rbind(cv_rmse1, cv_rmse3, rf_rmse, boost_rmse)
row.names(result2) <- c("Linear Model 1", "Linear Model 2", "Random Forest Model", "Boosted Model")
kable(result2, digits = 4, caption = "Cross Validation - Comparisons of the models in test set")

```

# Model Comparisons

The best model fit to predict the number of shares for the `r params$channel` channel is "need to automate this part".


```{r eval=TRUE}

```

The best model fit to predict the number of shares


# Automation

Automation is done with the modifications of the YAML header and the render function. 


